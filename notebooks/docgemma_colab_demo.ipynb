{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# DocGemma Connect - Colab Demo\n",
    "\n",
    "Medical AI assistant powered by MedGemma 4B with structured generation.\n",
    "\n",
    "**Requirements:**\n",
    "- GPU runtime (T4 or better)\n",
    "- HuggingFace account with MedGemma access\n",
    "- GitHub Personal Access Token (for private repo)\n",
    "\n",
    "**Setup:**\n",
    "1. Go to **Runtime > Change runtime type > T4 GPU**\n",
    "2. Click the **key icon** in the left sidebar to open Secrets\n",
    "3. Add `GITHUB_TOKEN` with your GitHub PAT\n",
    "4. Add `HF_TOKEN` with your HuggingFace token (optional, for auto-login)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-header"
   },
   "source": [
    "## 1. Install DocGemma from GitHub (Private Repo)\n",
    "\n",
    "Since this is a private repository, you need a GitHub Personal Access Token (PAT).\n",
    "\n",
    "**Create a PAT:** GitHub → Settings → Developer settings → Personal access tokens → Generate new token (classic) → Select `repo` scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Option 1: Use Colab Secrets (recommended)\n",
    "# Add your GitHub PAT as a secret named \"GITHUB_TOKEN\" in the Colab sidebar (key icon)\n",
    "from google.colab import userdata\n",
    "GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "!pip install -q git+https://{GITHUB_TOKEN}@github.com/galinilin/docgemma-connect.git\n",
    "\n",
    "# Option 2: Direct token (less secure - token visible in notebook)\n",
    "# !pip install -q git+https://ghp_yourtoken@github.com/galinilin/docgemma-connect.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auth-header"
   },
   "source": [
    "## 2. Authenticate with HuggingFace\n",
    "\n",
    "MedGemma requires approval. Request access at: https://huggingface.co/google/medgemma-1.5-4b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auth"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "# Option 1: Use Colab Secrets (if you added HF_TOKEN)\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Logged in via Colab secret\")\n",
    "except:\n",
    "    # Option 2: Interactive login (will prompt for token)\n",
    "    login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-header"
   },
   "source": [
    "## 3. Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load"
   },
   "outputs": [],
   "source": [
    "from docgemma import DocGemma\n",
    "\n",
    "# Initialize and load the model\n",
    "model = DocGemma()\n",
    "model.load()\n",
    "\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "print(f\"Using dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emergency-header"
   },
   "source": [
    "## 4. Emergency Classification\n",
    "\n",
    "Classify if a medical query is an emergency using constrained generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emergency"
   },
   "outputs": [],
   "source": [
    "# Test emergency classification\n",
    "test_cases = [\n",
    "    \"I have a mild headache\",\n",
    "    \"I'm experiencing severe chest pain and difficulty breathing\",\n",
    "    \"My child has a fever of 38C\",\n",
    "    \"I think I'm having a stroke, my face feels numb\",\n",
    "]\n",
    "\n",
    "print(\"Emergency Classification Results:\")\n",
    "print(\"-\" * 50)\n",
    "for case in test_cases:\n",
    "    result = model.classify_emergency(case)\n",
    "    print(f\"Input: {case}\")\n",
    "    print(f\"Result: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usertype-header"
   },
   "source": [
    "## 5. User Type Classification\n",
    "\n",
    "Detect if the user is a patient or medical expert based on their language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usertype"
   },
   "outputs": [],
   "source": [
    "# Test user type classification\n",
    "test_cases = [\n",
    "    \"My tummy hurts after eating\",\n",
    "    \"Patient presents with acute epigastric pain, possible peptic ulcer\",\n",
    "    \"What does my blood test result mean?\",\n",
    "    \"Differential diagnosis for elevated troponin with normal ECG?\",\n",
    "]\n",
    "\n",
    "print(\"User Type Classification Results:\")\n",
    "print(\"-\" * 50)\n",
    "for case in test_cases:\n",
    "    result = model.classify_user_type(case)\n",
    "    print(f\"Input: {case}\")\n",
    "    print(f\"Result: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generate-header"
   },
   "source": [
    "## 6. Free-form Generation\n",
    "\n",
    "Generate medical responses using the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate"
   },
   "outputs": [],
   "source": [
    "# Generate a medical response\n",
    "prompt = \"What are the common causes of headaches and when should I see a doctor?\"\n",
    "\n",
    "response = model.generate(prompt, max_new_tokens=256)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nResponse:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "structured-header"
   },
   "source": [
    "## 7. Structured Generation with Pydantic\n",
    "\n",
    "Generate responses that conform to a specific schema using Outlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "structured"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "# Define a structured output schema\n",
    "class TriageResult(BaseModel):\n",
    "    urgency: Literal[\"low\", \"medium\", \"high\", \"critical\"]\n",
    "    category: str\n",
    "    recommendation: str\n",
    "\n",
    "# Generate structured output\n",
    "prompt = \"\"\"Triage the following patient complaint and provide a structured assessment.\n",
    "\n",
    "Patient complaint: I've had a persistent cough for 2 weeks with some yellow mucus.\n",
    "\n",
    "Provide your assessment:\"\"\"\n",
    "\n",
    "result = model.generate_outlines(prompt, TriageResult)\n",
    "print(f\"Urgency: {result.urgency}\")\n",
    "print(f\"Category: {result.category}\")\n",
    "print(f\"Recommendation: {result.recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline-header"
   },
   "source": [
    "## 8. Combined Triage Pipeline Example\n",
    "\n",
    "A simple example of the decision-tree approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pipeline"
   },
   "outputs": [],
   "source": [
    "def triage_query(model, query: str) -> dict:\n",
    "    \"\"\"Simple triage pipeline using DocGemma's classification.\"\"\"\n",
    "    \n",
    "    # Step 1: Check for emergency\n",
    "    emergency_status = model.classify_emergency(query)\n",
    "    \n",
    "    if emergency_status == \"emergency\":\n",
    "        return {\n",
    "            \"status\": \"EMERGENCY\",\n",
    "            \"action\": \"Call emergency services immediately (911)\",\n",
    "            \"query\": query\n",
    "        }\n",
    "    \n",
    "    # Step 2: Classify user type\n",
    "    user_type = model.classify_user_type(query)\n",
    "    \n",
    "    # Step 3: Generate appropriate response\n",
    "    if user_type == \"patient\":\n",
    "        system_context = \"Respond in simple, easy-to-understand language for a patient.\"\n",
    "    else:\n",
    "        system_context = \"Respond with clinical terminology appropriate for a medical professional.\"\n",
    "    \n",
    "    full_prompt = f\"{system_context}\\n\\nQuery: {query}\"\n",
    "    response = model.generate(full_prompt, max_new_tokens=200)\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"non_emergency\",\n",
    "        \"user_type\": user_type,\n",
    "        \"response\": response,\n",
    "        \"query\": query\n",
    "    }\n",
    "\n",
    "# Test the pipeline\n",
    "queries = [\n",
    "    \"I have sudden severe chest pain radiating to my arm\",\n",
    "    \"What's the best way to treat a common cold?\",\n",
    "    \"Recommended prophylaxis for DVT in post-operative patients?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(\"=\" * 60)\n",
    "    result = triage_query(model, query)\n",
    "    for key, value in result.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
